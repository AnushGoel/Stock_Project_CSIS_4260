{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca3a568-cf84-4a07-a0af-b69f5665302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d5b475-45ab-479b-8fe1-81376f1e6f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_dataset.csv' with the actual path to your CSV file.\n",
    "csv_file = 'all_stocks_5yr.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(\"CSV data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79125646-c529-45f8-8a8c-1f93fe174a0b",
   "metadata": {},
   "source": [
    "## Convert and Save as Parquet (with Compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae7ecec-22fe-46c0-8a18-2f2be8f73e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully writt`en to Parquet with Snappy compression.\n"
     ]
    }
   ],
   "source": [
    "parquet_file = 'your_dataset_snappy.parquet'\n",
    "df.to_parquet(parquet_file, compression='snappy', index=False)\n",
    "print(\"Data successfully writt`en to Parquet with Snappy compression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f79084c-800e-4aa0-910a-4e956762f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_scale(df, factor):\n",
    "    \"\"\"Duplicate the DataFrame 'factor' times.\"\"\"\n",
    "    return pd.concat([df] * factor, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c10768-2319-4324-9738-48fd1515254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] \n",
    "scales = [1, 10, 100] \n",
    "scale_labels = [\"1x\", \"10x\", \"100x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dd3ced-2e29-4369-bead-aa996797ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor, label in zip(scales, scale_labels):\n",
    "    if factor == 1:\n",
    "        df_scaled = df\n",
    "    else:\n",
    "        df_scaled = simulate_scale(df, factor)\n",
    "    \n",
    "    # Define temporary filenames for the scaled CSV and Parquet files.\n",
    "    csv_filename = f'scaled_dataset_{label}.csv'\n",
    "    parquet_filename = f'scaled_dataset_{label}_snappy.parquet'\n",
    "    \n",
    "    # --- CSV Write Benchmark ---\n",
    "    start = time.time()\n",
    "    df_scaled.to_csv(csv_filename, index=False)\n",
    "    csv_write_time = time.time() - start\n",
    "    \n",
    "    # --- CSV Read Benchmark ---\n",
    "    start = time.time()\n",
    "    _ = pd.read_csv(csv_filename)\n",
    "    csv_read_time = time.time() - start\n",
    "    \n",
    "    # --- Parquet Write Benchmark (using Snappy compression) ---\n",
    "    start = time.time()\n",
    "    df_scaled.to_parquet(parquet_filename, compression='snappy', index=False)\n",
    "    parquet_write_time = time.time() - start\n",
    "    \n",
    "    # --- Parquet Read Benchmark ---\n",
    "    start = time.time()\n",
    "    _ = pd.read_parquet(parquet_filename)\n",
    "    parquet_read_time = time.time() - start\n",
    " \n",
    "    # --- Measure File Sizes (in MB) ---\n",
    "    csv_size = os.path.getsize(csv_filename) / (1024 * 1024)\n",
    "    parquet_size = os.path.getsize(parquet_filename) / (1024 * 1024)\n",
    "\n",
    "    # Store the results in a dictionary.\n",
    "    results.append({\n",
    "        \"Scale\": label,\n",
    "        \"CSV Write Time (s)\": round(csv_write_time, 2),\n",
    "        \"CSV Read Time (s)\": round(csv_read_time, 2),\n",
    "        \"Parquet Write Time (s)\": round(parquet_write_time, 2),\n",
    "        \"Parquet Read Time (s)\": round(parquet_read_time, 2),\n",
    "        \"CSV Size (MB)\": round(csv_size, 2),\n",
    "        \"Parquet Size (MB)\": round(parquet_size, 2)\n",
    "    })\n",
    "os.remove(csv_filename)\n",
    "os.remove(parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec14624-1686-4e7f-aece-546c0829db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Benchmarks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scale</th>\n",
       "      <th>CSV Write Time (s)</th>\n",
       "      <th>CSV Read Time (s)</th>\n",
       "      <th>Parquet Write Time (s)</th>\n",
       "      <th>Parquet Read Time (s)</th>\n",
       "      <th>CSV Size (MB)</th>\n",
       "      <th>Parquet Size (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1x</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.28</td>\n",
       "      <td>28.80</td>\n",
       "      <td>10.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10x</td>\n",
       "      <td>33.98</td>\n",
       "      <td>5.29</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.35</td>\n",
       "      <td>288.01</td>\n",
       "      <td>95.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100x</td>\n",
       "      <td>333.29</td>\n",
       "      <td>55.64</td>\n",
       "      <td>44.04</td>\n",
       "      <td>15.01</td>\n",
       "      <td>2880.05</td>\n",
       "      <td>951.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scale  CSV Write Time (s)  CSV Read Time (s)  Parquet Write Time (s)  \\\n",
       "0    1x                3.34               0.55                    0.53   \n",
       "1   10x               33.98               5.29                    4.38   \n",
       "2  100x              333.29              55.64                   44.04   \n",
       "\n",
       "   Parquet Read Time (s)  CSV Size (MB)  Parquet Size (MB)  \n",
       "0                   0.28          28.80              10.15  \n",
       "1                   1.35         288.01              95.35  \n",
       "2                  15.01        2880.05             951.71  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Benchmarks:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca3a58-f58f-4bbd-842c-66c9f912d51d",
   "metadata": {},
   "source": [
    "## Part A Summary:\n",
    "> Faster Read/Write: Parquet significantly outperforms CSV in both read and write times, especially as dataset size increases.\n",
    "\n",
    "> Smaller File Size: Parquet files are compressed and take up ∼35% of the space compared to CSV files at all scales.\n",
    "\n",
    "> Better Scaling: Parquet's efficiency becomes more evident at larger scales (10× and 100×), with faster operations and reduced storage requirements.\n",
    "\n",
    "> Recommendation: For large-scale datasets, Parquet is the better option for analytical workloads due to its speed and compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
